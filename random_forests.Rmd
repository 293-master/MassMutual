---
title: "Coding Exercises"
author: "Albert Y. Kim"
date: "Last updated on `r Sys.Date()`"
output:
  html_document: 
    df_print: kable
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    toc_float: 
      collapsed: false
---

<style>
h1{font-weight: 400;}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE, fig.width=16/2, fig.height=9/2)
library(tidyverse)
library(randomForest)
library(skimr)
library(ggraph)
library(igraph)
library(caret)
```



# Prepare data


```{r}
library(tidyverse)
library(randomForest)

# Load data and select only subset of variables:
train <- read_csv("https://rudeboybert.github.io/SDS293/static/train.csv") %>% 
  select(Id, CentralAir, Fireplaces, GrLivArea, YearBuilt, FullBath, SalePrice) %>% 
  # Convert to 1/0
  mutate(CentralAir = ifelse(CentralAir == "Y", 1, 0))
test <- read_csv("https://rudeboybert.github.io/SDS293/static/test.csv") %>% 
  select(Id, CentralAir, Fireplaces, GrLivArea, YearBuilt, FullBath) %>% 
  # Convert to 1/0
  mutate(CentralAir = ifelse(CentralAir == "Y", 1, 0))

# Transform outcome variable space:
train <- train %>% 
  mutate(logSalePrice = log(SalePrice + 1))

# Define model formula
model_formula <-
  "logSalePrice ~ CentralAir + Fireplaces + GrLivArea + YearBuilt + FullBath" %>%
  as.formula()

# Fit random forest:
model_rf <- randomForest(
  # Model formula
  form = model_formula,
  # Training data
  data = train, 
  # At each node of tree, number of features/predictor variables to randomly choose from
  # for splitting:
  mtry = 2,
  # Number of trees in your forest
  ntree = 100 
)

# Predict on test data
test <- test %>% 
  mutate(
    logSalePrice_hat = predict(object = model_rf, newdata = test),
    SalePrice_hat = exp(logSalePrice_hat) -1
  )
```



Here we use the `caret` package as a wrapper to the `randomForest` package's `randomForest()` function. The beauty of the `caret` is you can switch in a lot of different modeling methods easily; see a list of models [here](http://topepo.github.io/caret/available-models.html){target="_blank"}.


```{r}
# Specify 10-fold CV
fit_control <- trainControl(method = "cv", number = 10)

# Number of randomly chosen variables to split on at each node of tree
mtry <- 2:4
tunegrid <- expand.grid(.mtry=mtry)

# Fit model
set.seed(76)
model_rf <- caret::train(
  form = model_formula, 
  data = train, 
  # Set method to random forests:
  method = "rf",
  metric = "RMSE",
  trControl = fit_control,
  tuneGrid = tunegrid
  )
model_rf

# Make predictions in transformed space:
logSalePrice_hat <- predict(model_rf, test)
logSalePrice_hat

# Return predictions to original space by undoing transformation:
SalePrice_hat <- exp(logSalePrice_hat) - 1
SalePrice_hat

# Submit to Kaggle:
submission_rf <- test %>%
  select(Id) %>%
  mutate(SalePrice = SalePrice_hat)
write_csv(submission_rf, "submission_rf.csv")
```





# Bias-Variance Trade-off

Recall the following slide from the presentation on Tuesday:

<center>
![](https://rudeboybert.github.io/SDS293/static/images/model_performance.png){ width=500px }
</center>

Why does the orange curve (the model error when you fit a model to training but evaluate it's prediction performance on new independent test data) have that U-shape? Because of the "bias-variance" trade-off. 

$$
\mbox{MSE}\left[\widehat{f}(x)\right] = \mbox{Var}\left[\widehat{f}(x)\right] +
\left(\mbox{Bias}\left[\widehat{f}(x)\right]\right)^2 + \sigma^2
$$

where $\widehat{y} = \widehat{f}(x)$ and $y=f(x)+\epsilon$ with $\mathbb{E}\left[\epsilon\right] = 0$ and $\mbox{Var}[\epsilon] = \sigma$. For more info, read this [blog post](http://scott.fortmann-roe.com/docs/BiasVariance.html){target="_blank"}.



