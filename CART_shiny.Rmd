
---
title: "CART tuning parameter"
author: "Albert Y. Kim"
date: "Last updated on `r Sys.Date()`"
output:
  html_document:
    theme: cosmo
    highlight: tango
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: kable
runtime: shiny
---

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(shiny)
library(rpart)
iris <- iris %>%
  as_tibble() %>%
  # Add ID column:
  mutate(ID = 1:n()) %>% 
  select(ID, Species, Sepal.Length, Sepal.Width)
model_formula <- as.formula(Species ~ Sepal.Length + Sepal.Width)
```



## Tuning parameter

Say you have a categorical outcome variable $y$ with $K$ levels/possible values. For a given "complexity parameter" $\alpha$, CART returns the subtree that minimizes the following equation:

$$
G  + \alpha |T| = \sum_{m=1}^{|T|}\sum_{k=1}^{K}\left(\widehat{p}_{mk}(1-\widehat{p}_{mk})\right) + \alpha |T|
$$

where 

* The $k$ different levels of the categorical outcome variable are indexed with $k=1, \ldots, K$.
* $T$ is the tree. $|T|$ is the "cardinality" of the tree (a term from set theory); it denotes the "complexity" of the tree as measured by the number of terminal nodes AKA leafs. These leafs are indexed with $m=1, \ldots, |T|$
* $\widehat{p}_{mk}$ are the fitted probabilities for each of the $k = 1, \ldots, K$ categories at a particular leaf $m$
* G = ["Gini impurity"](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) is a measure of "fit" where smaller values indicate better fit (see ISLR p.312).
* $\alpha$ here is the "complexity parameter", which you can set using the `cp` argument in `rpart.control()`.

For example using $\alpha$ = `cp=0.01`, which is the default value (see `?rpart.control`), the code is:

```{r, eval=FALSE}
# Define tree
tree_parameters <- rpart.control(maxdepth = 3, cp = 0.01)
model_CART <- rpart(model_formula, data = iris, control = tree_parameters)

# Plot
plot(model_CART, margin=0.25)
text(model_CART, use.n = TRUE)
title("Predicting iris species using sepal length & width")
box()
```


## Varying the tuning parameter

Let's vary the tuning parameter $\alpha$ using the slider below:

```{r, echo=FALSE}
inputPanel(
  sliderInput("cp", label = "Tuning parameter alpha:", min = 0, max = 0.5, value = 0.1, step = 0.001)
)
```


Notice how the actual splits at each fork don't change, but rather only the
depth of the tree changes for different values of the "knob" $\alpha$ AKA the
tuning parameter that controls the complexity of the model. 

```{r, echo=FALSE}
renderPlot({
  # Create tree
  tree_parameters <- rpart.control(maxdepth = 3, cp=input$cp)
  model_CART <- rpart(model_formula, data = iris, control=tree_parameters)
  
  # Plot
  plot(model_CART, margin=0.25)
  text(model_CART, use.n = TRUE)
  title("Predicting iris species using sepal length & width")
  box()
}, height=900/2, width=1600/2)
```


## For numerical outcomes

When the outcome variable $y$ is numerical, the formula that gets minimized is (see ISLR p.309):

$$
RSS + \alpha |T| = \sum_{m=1}^{|T|}\sum_{x_i \in R_m} \left(y_i - \widehat{y}_{R_m}\right)^2 + \alpha |T|
$$

where RSS = "Residual sum of squares" is a measure of "fit": as the $\widehat{y}$ more closely match the $y_i$, the RSS gets closer to 0. 
