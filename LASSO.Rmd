
---
title: "LASSO Regularization"
author: "Albert Y. Kim"
date: "Last updated on `r Sys.Date()`"
output:
  html_document:
    theme: cosmo
    highlight: tango
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: kable
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.width = 10
)

library(tidyverse)
library(broom)
library(stringr)
library(knitr)
library(moderndive)
library(glmnet)
library(ISLR)
library(plotly)

set.seed(76)
```


# Data

Note: this is the same data as you in Lec06 on Wed 2/13 in the <a href="static/methods/regression/regression_v2.Rmd" download>`regression_v2.Rmd`</a> Shiny App. For a more thorough EDA please revisit the Shiny App.

Let's consider data for $i=1, \ldots, 400$ individuals' credit card debt. Note this data was simulated and is not real. 

* $y_i$: Credit card balance i.e. credit card debt
* $x_{1,i}$: Income in $10K
* $x_{2,i}$: Credit limit in $

```{r}
credit <- Credit %>%
  select(Balance, Income, Limit)
```

Let's also consider two models for credit card `Balance` i.e. credit card debt

1. **Naive Model**: Uses no predictor information and thus the fitted value $\widehat{y}$ is just the mean $\overline{y}$
1. **Multiple regression model**: Uses both predictors `Limit` and `Income`.



***



# 1. Naive Model

Say we use no predictor information. This corresponds to the following true model $f()$ and error component $\epsilon$.

$$
\begin{aligned}
y &= f(\vec{x}) + \epsilon\\
y &= \beta_0 + \epsilon\\
\mbox{Balance} &= \beta_0 + \epsilon
\end{aligned}
$$

In other words there is only an intercept term. Since the mean credit card balance AKA credit card debt $\bar{y}$ is:

```{r}
mean(credit$Balance)
```

We'll estimate/approximate $f()$ with the following fitted model $\widehat{f}()$:

$$
\begin{aligned}
\widehat{y} &= \widehat{f}(\vec{x})\\
\widehat{y} &= \widehat{\beta}_0\\
\widehat{\mbox{Balance}} &= \widehat{\beta}_0 \\
\widehat{\mbox{Balance}} &= \overline{y}
\end{aligned}
$$

In other words, think of the above fitted model $\widehat{f}(\vec{x})$ as a **minimally viable model**, in other words a "null" model, in other words a "basic baseline model". Using this model, our prediction $\widehat{y}$ of an individual's credit bard balance using no predictor information would be $\bar{y}$ = \$520.01. Let' visualize this in a histogram:

```{r, fig.width=8}
ggplot(credit, aes(x = Balance)) +
  geom_histogram(binwidth = 100, boundary = 0) +
  labs(x = "y = Balance ($)", title = "Histogram of outcome variable: credit card balance") +
  geom_vline(xintercept = mean(credit$Balance), col = "red", size = 1)
```

Surely we can do better than this however! We are not using any of the information contained in the predictor variables $x_1$ `Income` and $x_2$ credit `Limit`. In other words, we are predicting $520.01 as the credit card debt irregardless of the individual's income and credit limit. 



***



# 2. Multiple Regression Model

Let's now fit a multiple linear regression model with two predictors:

$$
\begin{aligned}
y &= f(\vec{x}) + \epsilon\\
y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon\\
\mbox{Balance} &= \beta_0 + \beta_1\mbox{Income} + \beta_2\mbox{Limit} + \epsilon
\end{aligned}
$$

Kind of like before, we'll estimate/approximate $f()$ with a fitted model $\widehat{f}()$ based on the fitted values of the $\beta$'s from the regression table:

```{r}
model_lm <- lm(Balance ~ Income + Limit, data = credit)
model_lm %>% 
  get_regression_table()
```

Hence:

$$
\begin{aligned}
\widehat{y} &= \widehat{f}(\vec{x})\\
\widehat{y} &= \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_2x_2\\
\widehat{\mbox{Balance}} &= \widehat{\beta}_0 + \widehat{\beta}_1\mbox{Income} + \widehat{\beta}_2\mbox{Limit}\\
\widehat{\mbox{Balance}} &= -385.179 - 7.663  \cdot \mbox{Income} + 0.264 \cdot \mbox{Limit}\\
\end{aligned}
$$

Recall we visualized the corresponding 3D scatterplot and regression plane in the <a href="static/methods/regression/regression_v2.Rmd" download>`regression_v2.Rmd`</a> Shiny App.



***



# Shrinking $\beta$ Coefficients via LASSO

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10}
model_formula <- as.formula("Balance ~ Income + Limit")
X <- model.matrix(model_formula, data = credit)[, -1]
y <- credit$Balance
lambda_values <- 10^seq(-3, 10, by=0.05)
model_ridge <- glmnet(X, y, alpha = 1, lambda = lambda_values)
  coefficients <-
    model_ridge %>%
    tidy(return_zeros = TRUE) %>%
    tbl_df() %>%
    # lambda's on x-axis are better viewed on a log-scale:
    mutate(log10_lambda = log10(lambda)) %>%
    select(term, estimate, log10_lambda)
```

**Recall**: 

We now set up a search range of $\lambda$ values to consider in the slider for the Shiny app. Note 
however we don't vary things on a $\lambda$-scale, but rather a $\log_{10}(\lambda)$-scale. Here
is our search range:

* Values of $\log_{10}$ in between $(-3, 10)$
* i.e. Values of $\lambda$ in between $(10^{-3}, 10^{10})$ = (0.001, 10,000,000,000)

**Observe**: As we vary $\log_{10}(\lambda)$, we see that the
LASSO $\beta$ coefficients change. Note the plot does not show how the intercept $\beta_0$ varies, but
the table does.

```{r,echo=FALSE}
inputPanel(
  sliderInput("lambda", label = "log10(lambda)", min = -3, max = 10, value = 1, step = 0.05)
)
```

```{r,echo=FALSE}
renderPlot({
  coefficients %>% 
    filter(term != "(Intercept)") %>% 
    ggplot(aes(x=log10_lambda, y=estimate, col=term)) +
    geom_line() +
    geom_vline(xintercept = input$lambda, linetype="dashed") +
    labs(x="log10(lambda)", y="estimate of coefficient")
})

renderTable({
  coefficients %>% 
    mutate(dist = abs(log10_lambda - input$lambda)) %>% 
    filter(dist == min(dist)) %>% 
    rename(`LASSO Estimate` = estimate) %>% 
    left_join(tidy(model_lm), by="term") %>% 
    rename(`lm Estimate` = estimate) %>% 
    select(term, `LASSO Estimate`, `lm Estimate`)
  
}, rownames = FALSE)
```

**Question**: When we penalize the coefficients very strictly via a high value 
of $\lambda$, note that both slope coefficients $\beta_1$ and $\beta_2$ get
shrunk to 0. What does the value of intercept coefficient $\beta_0$ correspond
to? Hint: We are using no predictor information when $\beta_1=0$ and $\beta_2=0$.





# What Happens at a Particular $\lambda$?

For example at $\log_{10}(\lambda) = 1$, or equivalently at $\lambda = 10^1 = 10$, we have:

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, cache=TRUE}
lambda_knob <- 1

coefficents2 <- coefficients %>% 
  filter(log10_lambda == lambda_knob) %>%
  select(term, estimate)

coefficents2 %>% 
  knitr::kable(digits=2)
```

and hence we obtain fitted values using:

$$
\widehat{y} = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_2x_2\\
\widehat{\mbox{Balance}} = -348.11 - 6.29 \cdot \mbox{Income} + 0.24\cdot\mbox{Limit}\\
$$

But where did $\left(\widehat{\beta}_0, \widehat{\beta}_1, \widehat{\beta}_2 
\right) = \left(-348.11, -6.29, 0.24\right)$ come from? How was this computed?
Recall the main optimization problem for the LASSO:

$$
\begin{align}
&\min_{\widehat{\beta}_0, \ldots, \widehat{\beta}_p} \left( \sum_{i=1}^n \left(y_i -\widehat{y}_i\right)^2 + \lambda \sum_{j=1}^p \left|\widehat{\beta}_j\right| \right)\\
\mbox{AKA } &\min_{\widehat{\beta}_0, \ldots, \widehat{\beta}_p} \left( \mbox{RSS} + \mbox{shrinkage penalty} \right)
\end{align}
$$

In other words and in our case, we want to find the $\left(\widehat{\beta}_0, \widehat{\beta}_1,
\widehat{\beta}_2\right)$ combination such that the following value is minimized (let's call it Total):

$$
\mbox{Total} = \sum_{i=1}^{n} \left(\mbox{Balance}_i -\widehat{\mbox{Balance}}_i\right)^2 + \lambda \left( \left|\widehat{\beta}_1\right| + \left|\widehat{\beta}_2\right|\right)\\
$$

where

$$
\widehat{\mbox{Balance}}_i = \widehat{\beta}_0 + \widehat{\beta}_1\mbox{Income}_i + \widehat{\beta}_2\mbox{Limit}_i\\
$$

How does `glmnet()` find these values? It uses *numerical optimization* 
techniques, but they are beyond the scope of this class. If you are interested 
in learning these however, you need a working knowledge of calculus, optimization, and numerical analysis.
